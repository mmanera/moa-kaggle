{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "moa_baseline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "Xq_3iUWH7jtU"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn import model_selection"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hmWPUGWeX3P",
        "outputId": "efcefc7f-61ca-402b-c836-f9512e68496c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "NDLO-u0X7jtY"
      },
      "source": [
        "train_features = pd.read_csv('/content/drive/My Drive/moa/train_features.csv')\n",
        "train_targets_scored = pd.read_csv('/content/drive/My Drive/moa/train_targets_scored.csv')\n",
        "sample_submission = pd.read_csv('/content/drive/My Drive/moa/sample_submission.csv')\n",
        "test_features = pd.read_csv('/content/drive/My Drive/moa/test_features.csv')"
      ],
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRbFeNtXo5WP"
      },
      "source": [
        "GENES = [col for col in train_features.columns if col.startswith('g-')]\n",
        "CELLS = [col for col in train_features.columns if col.startswith('c-')]"
      ],
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7B5ZmqYar3j",
        "outputId": "5c354252-2dcb-4a22-ce0d-64c3ea401cfa"
      },
      "source": [
        "train_features[\"cp_type\"].value_counts()\n",
        "# Can we drop cp_type column? ctl_vehicle is 8% from total."
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "trt_cp         21948\n",
              "ctl_vehicle     1866\n",
              "Name: cp_type, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hq7sWS3nwQlr"
      },
      "source": [
        "train = train_features.merge(train_targets_scored, on='sig_id')\n",
        "test = test_features\n",
        "# If we choose to drop train_features[train['cp_type']=='ctl_vehicle'], uncomment.\n",
        "# train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
        "# test = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
        "\n",
        "target = train[train_targets_scored.columns]\n",
        "train = train.drop('cp_type', axis=1)               # train[\"cp_type\"].unique() = 'trt_cp'. We cant pass cp_type without encode.\n",
        "train = train.drop('sig_id', axis=1)\n",
        "\n",
        "# target                      # 23814 rows Ã— 207 columns. # Its actually the same as train_targets_scored, if we didnt preprocess anythig."
      ],
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zN3HG4X8ptt8"
      },
      "source": [
        "class MoADataset:\n",
        "    def __init__(self, features, targets):\n",
        "        self.features = features\n",
        "        self.targets = targets\n",
        "        \n",
        "    def __len__(self):              # len() will use the __len__ method if present to get your object for its length.  \n",
        "        return (self.features.shape[0])\n",
        "    \n",
        "    def __getitem__(self, idx):     # docs: https://docs.python.org/3/reference/datamodel.html#object.__getitem__. In this case returns a dict.\n",
        "        dct = { \n",
        "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),       # ex: np_array[0, :] -> [1,2]\n",
        "            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n",
        "        }\n",
        "        return dct\n",
        "    \n",
        "class TestDataset:\n",
        "    def __init__(self, features):\n",
        "        self.features = features\n",
        "        \n",
        "    def __len__(self):\n",
        "        return (self.features.shape[0])\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        dct = {\n",
        "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n",
        "        }\n",
        "        return dct\n",
        "    "
      ],
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA1Xa_9Rp8Hz"
      },
      "source": [
        "def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n",
        "    model.train()\n",
        "    final_loss = 0\n",
        "    \n",
        "    for data in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        inputs, targets = data['x'].to(device), data['y'].to(device)        # Asks for the value of \"x\" and \"y\" keys.\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        \n",
        "        final_loss += loss.item()\n",
        "        \n",
        "    final_loss /= len(dataloader)\n",
        "    \n",
        "    return final_loss   \n",
        "\n",
        "def valid_fn(model, loss_fn, dataloader, device):\n",
        "    model.eval()\n",
        "    final_loss = 0\n",
        "    valid_preds = []\n",
        "    \n",
        "    for data in dataloader:\n",
        "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        \n",
        "        final_loss += loss.item()\n",
        "        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
        "        \n",
        "    final_loss /= len(dataloader)\n",
        "    valid_preds = np.concatenate(valid_preds)\n",
        "    \n",
        "    return final_loss, valid_preds\n",
        "\n",
        "def inference_fn(model, dataloader, device):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    \n",
        "    for data in dataloader:\n",
        "        inputs = data['x'].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs)\n",
        "        \n",
        "        preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
        "        \n",
        "    preds = np.concatenate(preds)\n",
        "    \n",
        "    return preds"
      ],
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lf8uupETxeol"
      },
      "source": [
        "# process_data(data) uses get_dummies() to create cp_time: 24, 48, 72. cp_dose: D1, D2\n",
        "def process_data(data):\n",
        "    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])              \n",
        "   \n",
        "    return data"
      ],
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3nEUtlOxxVS"
      },
      "source": [
        "# Simply target without id.\n",
        "target_cols = target.drop('sig_id', axis=1).columns.tolist()\n",
        "# We use this comprehension to take into account the dummies created by our process_data()\n",
        "feature_cols = [col for col in process_data(train).columns if col not in target_cols]"
      ],
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sQxSBXotzRb"
      },
      "source": [
        "# HyperParameters\n",
        "DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "EPOCHS = 2\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 1e-3\n",
        "WEIGHT_DECAY = 1e-5\n",
        "NFOLDS = 5\n",
        "EARLY_STOPPING_STEPS = 10\n",
        "EARLY_STOP = False\n",
        "\n",
        "num_features=len(feature_cols)\n",
        "num_targets=len(target_cols)\n",
        "hidden_size=1024"
      ],
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPVBzw_sqIfT"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, num_features, num_targets, hidden_size):\n",
        "        super(Model, self).__init__()\n",
        "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n",
        "        \n",
        "        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n",
        "        \n",
        "        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n",
        "        self.dropout3 = nn.Dropout(0.5)\n",
        "        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.batch_norm1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.dense1(x))\n",
        "        \n",
        "        x = self.batch_norm2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = F.relu(self.dense2(x))\n",
        "        \n",
        "        x = self.batch_norm3(x)\n",
        "        x = self.dropout3(x)\n",
        "        x = self.dense3(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUb7Yp2Weq9X"
      },
      "source": [
        "folds = train.copy()\n",
        "folds = folds.sample(frac=1).reset_index(drop=True)             # to randomize\n",
        "kf = model_selection.KFold(n_splits=5)\n",
        "for fold, (t_idx, v_idx) in enumerate(kf.split(X=folds)):\n",
        "    folds.loc[v_idx, 'kfold'] = fold\n",
        "folds['kfold'] = folds['kfold'].astype(int)           # Otherwise 0.0, 1.0, 2.0, 3.0...\n",
        "\n",
        "train = process_data(folds)\n",
        "\n",
        "test_df = process_data(test)\n",
        "x_test  = test_df[feature_cols].values\n",
        "test_dataset = TestDataset(x_test)\n",
        "testloader = torch.utils.data.DataLoader(test_dataset)\n",
        "\n",
        "predictions = np.zeros((3982,206))"
      ],
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlLZAAxityeV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdeb78ec-d04e-4ade-bcc5-8a7fa2fcaf94"
      },
      "source": [
        "def run_training(fold):\n",
        "    trn_idx = train[train['kfold'] != fold].index\n",
        "    val_idx = train[train['kfold'] == fold].index\n",
        "\n",
        "    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n",
        "    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n",
        "\n",
        "    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n",
        "    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n",
        "\n",
        "    train_dataset = MoADataset(x_train, y_train)\n",
        "    valid_dataset = MoADataset(x_valid, y_valid)\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)      # It has a len of 187. 187 * 128(BATCH_SIZE) = 23936. Contains the 23814 rows of the train_df.\n",
        "    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    model = Model(\n",
        "        num_features=num_features,\n",
        "        num_targets=num_targets,\n",
        "        hidden_size=hidden_size,\n",
        "    )\n",
        "\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3,\n",
        "                                                max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n",
        "    best_loss = np.inf                  # Represents a positive infinite\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        train_loss = train_fn(model, optimizer, scheduler, loss_fn, trainloader, DEVICE)\n",
        "        print(f\"EPOCH: {epoch}, train_loss: {train_loss}\")\n",
        "        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n",
        "        print(f\"EPOCH: {epoch}, valid_loss: {valid_loss}\")\n",
        "        \n",
        "        if valid_loss < best_loss:\n",
        "            print(f\"updating best model on Fold={fold}\") \n",
        "            best_loss = valid_loss\n",
        "            torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n",
        "    \n",
        "    model = Model(\n",
        "        num_features=num_features,\n",
        "        num_targets=num_targets,\n",
        "        hidden_size=hidden_size,\n",
        "    )\n",
        "    \n",
        "    model.load_state_dict(torch.load(f\"FOLD{fold}_.pth\"))\n",
        "    model.to(DEVICE)\n",
        "    \n",
        "    fold_preds = inference_fn(model, testloader, DEVICE)\n",
        "    global predictions\n",
        "    predictions = predictions + fold_preds\n",
        "      \n",
        "for run_k_fold in range(5):              # 5 folds\n",
        "    run_training(run_k_fold)\n",
        "\n",
        "predictions /= 5"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH: 0, train_loss: 0.14032905092525402\n",
            "EPOCH: 0, valid_loss: 0.01896585004502221\n",
            "updating best model on Fold=0\n",
            "EPOCH: 1, train_loss: 0.018855824510273116\n",
            "EPOCH: 1, valid_loss: 0.018226633853230038\n",
            "updating best model on Fold=0\n",
            "EPOCH: 0, train_loss: 0.14102164883441573\n",
            "EPOCH: 0, valid_loss: 0.01894737917341684\n",
            "updating best model on Fold=1\n",
            "EPOCH: 1, train_loss: 0.018950409087158688\n",
            "EPOCH: 1, valid_loss: 0.017634862445686992\n",
            "updating best model on Fold=1\n",
            "EPOCH: 0, train_loss: 0.1388026420777076\n",
            "EPOCH: 0, valid_loss: 0.0197015078248162\n",
            "updating best model on Fold=2\n",
            "EPOCH: 1, train_loss: 0.018738220152038857\n",
            "EPOCH: 1, valid_loss: 0.018097600875128256\n",
            "updating best model on Fold=2\n",
            "EPOCH: 0, train_loss: 0.1390480728532444\n",
            "EPOCH: 0, valid_loss: 0.019028457126727228\n",
            "updating best model on Fold=3\n",
            "EPOCH: 1, train_loss: 0.019025844903100258\n",
            "EPOCH: 1, valid_loss: 0.018177134818152377\n",
            "updating best model on Fold=3\n",
            "EPOCH: 0, train_loss: 0.14043975121422902\n",
            "EPOCH: 0, valid_loss: 0.01888782190355031\n",
            "updating best model on Fold=4\n",
            "EPOCH: 1, train_loss: 0.018727022660708668\n",
            "EPOCH: 1, valid_loss: 0.017980590217599745\n",
            "updating best model on Fold=4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_g9DA4RYwgV"
      },
      "source": [
        "sample_submission = pd.read_csv('/content/drive/My Drive/moa/sample_submission.csv')"
      ],
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GzfJHx8feli"
      },
      "source": [
        "y = pd.DataFrame(data=predictions)\n",
        "y.columns = target_cols"
      ],
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQ3ChHg1YyUc"
      },
      "source": [
        "sub = sample_submission.drop(columns=target_cols)\n",
        "frames = [sub, y]\n",
        "sub = pd.concat(frames, axis=1)"
      ],
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZuZfC_ocmKT",
        "outputId": "1847e7c9-f014-45c3-c368-31123afbc442"
      },
      "source": [
        "sample_submission.shape"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3982, 207)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdBp-IL_W2sJ",
        "outputId": "b66d27cf-3bd0-4605-8e2b-4dd301a62b3a"
      },
      "source": [
        "submission = sub.to_csv(\"submission.csv\",index=False)\n",
        "teste = pd.read_csv(\"submission.csv\")\n",
        "teste.shape"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3982, 207)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MTWdo0NbKZD"
      },
      "source": [
        ""
      ],
      "execution_count": 207,
      "outputs": []
    }
  ]
}
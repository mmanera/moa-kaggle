{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "moa_baseline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "Xq_3iUWH7jtU"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn import model_selection"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hmWPUGWeX3P",
        "outputId": "95df0fea-b387-491d-f1d0-515485334a25"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "NDLO-u0X7jtY"
      },
      "source": [
        "train_features = pd.read_csv('/content/drive/My Drive/moa/train_features.csv')\n",
        "train_targets_scored = pd.read_csv('/content/drive/My Drive/moa/train_targets_scored.csv')\n",
        "sample_submission = pd.read_csv('/content/drive/My Drive/moa/sample_submission.csv')\n",
        "test_features = pd.read_csv('/content/drive/My Drive/moa/test_features.csv')"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRbFeNtXo5WP"
      },
      "source": [
        "GENES = [col for col in train_features.columns if col.startswith('g-')]\n",
        "CELLS = [col for col in train_features.columns if col.startswith('c-')]"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7B5ZmqYar3j",
        "outputId": "1e7633aa-2baf-4ae4-ccd4-fe92928a362e"
      },
      "source": [
        "train_features[\"cp_type\"].value_counts()\n",
        "# Can we drop cp_type column? ctl_vehicle is 8% from total."
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "trt_cp         21948\n",
              "ctl_vehicle     1866\n",
              "Name: cp_type, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hq7sWS3nwQlr"
      },
      "source": [
        "train = train_features.merge(train_targets_scored, on='sig_id')\n",
        "test = test_features\n",
        "# If we choose to drop train_features[train['cp_type']=='ctl_vehicle'], uncomment.\n",
        "# train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
        "# test = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
        "\n",
        "target = train[train_targets_scored.columns]\n",
        "train = train.drop('cp_type', axis=1)               # train[\"cp_type\"].unique() = 'trt_cp'. We cant pass cp_type without encode.\n",
        "train = train.drop('sig_id', axis=1)\n",
        "\n",
        "# target                      # 23814 rows Ã— 207 columns. # Its actually the same as train_targets_scored, if we didnt preprocess anythig."
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zN3HG4X8ptt8"
      },
      "source": [
        "class MoADataset:\n",
        "    def __init__(self, features, targets):\n",
        "        self.features = features\n",
        "        self.targets = targets\n",
        "        \n",
        "    def __len__(self):              # len() will use the __len__ method if present to get your object for its length.  \n",
        "        return (self.features.shape[0])\n",
        "    \n",
        "    def __getitem__(self, idx):     # docs: https://docs.python.org/3/reference/datamodel.html#object.__getitem__. In this case returns a dict.\n",
        "        dct = { \n",
        "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),       # ex: np_array[0, :] -> [1,2]\n",
        "            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n",
        "        }\n",
        "        return dct\n",
        "    \n",
        "class TestDataset:\n",
        "    def __init__(self, features):\n",
        "        self.features = features\n",
        "        \n",
        "    def __len__(self):\n",
        "        return (self.features.shape[0])\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        dct = {\n",
        "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n",
        "        }\n",
        "        return dct\n",
        "    "
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA1Xa_9Rp8Hz"
      },
      "source": [
        "def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n",
        "    model.train()\n",
        "    final_loss = 0\n",
        "    \n",
        "    for data in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        inputs, targets = data['x'].to(device), data['y'].to(device)        # Asks for the value of \"x\" and \"y\" keys.\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        \n",
        "        final_loss += loss.item()\n",
        "        \n",
        "    final_loss /= len(dataloader)\n",
        "    \n",
        "    return final_loss   \n",
        "\n",
        "def valid_fn(model, loss_fn, dataloader, device):\n",
        "    model.eval()\n",
        "    final_loss = 0\n",
        "    valid_preds = []\n",
        "    \n",
        "    for data in dataloader:\n",
        "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        \n",
        "        final_loss += loss.item()\n",
        "        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
        "        \n",
        "    final_loss /= len(dataloader)\n",
        "    valid_preds = np.concatenate(valid_preds)\n",
        "    \n",
        "    return final_loss, valid_preds\n",
        "\n",
        "def inference_fn(model, dataloader, device):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    \n",
        "    for data in dataloader:\n",
        "        inputs = data['x'].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs)\n",
        "        \n",
        "        preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
        "        \n",
        "    preds = np.concatenate(preds)\n",
        "    \n",
        "    return preds"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lf8uupETxeol"
      },
      "source": [
        "# process_data(data) uses get_dummies() to create cp_time: 24, 48, 72. cp_dose: D1, D2\n",
        "def process_data(data):\n",
        "    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])              \n",
        "   \n",
        "    return data"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3nEUtlOxxVS"
      },
      "source": [
        "# Simply target without id.\n",
        "target_cols = target.drop('sig_id', axis=1).columns.tolist()\n",
        "# We use this comprehension to take into account the dummies created by our process_data()\n",
        "feature_cols = [col for col in process_data(train).columns if col not in target_cols]"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sQxSBXotzRb"
      },
      "source": [
        "# HyperParameters\n",
        "DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "EPOCHS = 2\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 1e-3\n",
        "WEIGHT_DECAY = 1e-5\n",
        "NFOLDS = 5\n",
        "EARLY_STOPPING_STEPS = 10\n",
        "EARLY_STOP = False\n",
        "\n",
        "num_features=len(feature_cols)\n",
        "num_targets=len(target_cols)\n",
        "hidden_size=1024"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPVBzw_sqIfT"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, num_features, num_targets, hidden_size):\n",
        "        super(Model, self).__init__()\n",
        "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n",
        "        \n",
        "        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n",
        "        \n",
        "        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n",
        "        self.dropout3 = nn.Dropout(0.5)\n",
        "        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.batch_norm1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.dense1(x))\n",
        "        \n",
        "        x = self.batch_norm2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = F.relu(self.dense2(x))\n",
        "        \n",
        "        x = self.batch_norm3(x)\n",
        "        x = self.dropout3(x)\n",
        "        x = self.dense3(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUb7Yp2Weq9X"
      },
      "source": [
        "folds = train.copy()\n",
        "folds = folds.sample(frac=1).reset_index(drop=True)             # to randomize\n",
        "kf = model_selection.KFold(n_splits=5)\n",
        "for fold, (t_idx, v_idx) in enumerate(kf.split(X=folds)):\n",
        "    folds.loc[v_idx, 'kfold'] = fold\n",
        "folds['kfold'] = folds['kfold'].astype(int)           # Otherwise 0.0, 1.0, 2.0, 3.0...\n",
        "\n",
        "train = process_data(folds)\n",
        "\n",
        "test_df = process_data(test)\n",
        "x_test  = test_df[feature_cols].values\n",
        "test_dataset = TestDataset(x_test)\n",
        "testloader = torch.utils.data.DataLoader(test_dataset)\n",
        "\n",
        "predictions = np.zeros((3982,206))"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlLZAAxityeV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0b7b891-15a2-40a6-f51f-1ec154357730"
      },
      "source": [
        "def run_training(fold):\n",
        "    trn_idx = train[train['kfold'] != fold].index\n",
        "    val_idx = train[train['kfold'] == fold].index\n",
        "\n",
        "    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n",
        "    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n",
        "\n",
        "    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n",
        "    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n",
        "\n",
        "    train_dataset = MoADataset(x_train, y_train)\n",
        "    valid_dataset = MoADataset(x_valid, y_valid)\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)      # It has a len of 187. 187 * 128(BATCH_SIZE) = 23936. Contains the 23814 rows of the train_df.\n",
        "    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    model = Model(\n",
        "        num_features=num_features,\n",
        "        num_targets=num_targets,\n",
        "        hidden_size=hidden_size,\n",
        "    )\n",
        "\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3,\n",
        "                                                max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n",
        "    best_loss = np.inf                  # Represents a positive infinite\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        train_loss = train_fn(model, optimizer, scheduler, loss_fn, trainloader, DEVICE)\n",
        "        print(f\"EPOCH: {epoch}, train_loss: {train_loss}\")\n",
        "        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n",
        "        print(f\"EPOCH: {epoch}, valid_loss: {valid_loss}\")\n",
        "        \n",
        "        if valid_loss < best_loss:\n",
        "            print(f\"updating best model on Fold={fold}\") \n",
        "            best_loss = valid_loss\n",
        "            # oof[val_idx] = valid_preds\n",
        "            torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n",
        "    \n",
        "    fold_preds = inference_fn(model, testloader, DEVICE)\n",
        "    global predictions\n",
        "    predictions = predictions + fold_preds\n",
        "    print(predictions)    \n",
        "      \n",
        "for run_k_fold in range(5):              # 5 folds\n",
        "    run_training(run_k_fold)\n",
        "\n",
        "predictions /= 5"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH: 0, train_loss: 0.1403922475811919\n",
            "EPOCH: 0, valid_loss: 0.01940150406995886\n",
            "updating best model on Fold=0\n",
            "EPOCH: 1, train_loss: 0.018808677955061796\n",
            "EPOCH: 1, valid_loss: 0.018533614660172087\n",
            "updating best model on Fold=0\n",
            "[[0.00140438 0.00181455 0.00177666 ... 0.00169914 0.00157534 0.00210482]\n",
            " [0.0010858  0.00136895 0.00182723 ... 0.00149622 0.00329186 0.00186699]\n",
            " [0.00087761 0.00052244 0.00103695 ... 0.00087652 0.00130898 0.00114798]\n",
            " ...\n",
            " [0.00114555 0.00100715 0.00110471 ... 0.00159549 0.00307395 0.00131794]\n",
            " [0.00126276 0.00106065 0.00151006 ... 0.00107573 0.00116278 0.00211597]\n",
            " [0.00092951 0.00114269 0.00131762 ... 0.00140431 0.00208056 0.00141018]]\n",
            "EPOCH: 0, train_loss: 0.140029293641248\n",
            "EPOCH: 0, valid_loss: 0.01926086048938726\n",
            "updating best model on Fold=1\n",
            "EPOCH: 1, train_loss: 0.018792859334873672\n",
            "EPOCH: 1, valid_loss: 0.018364380187305965\n",
            "updating best model on Fold=1\n",
            "[[0.00268737 0.00354354 0.00328327 ... 0.00330219 0.00365534 0.00412797]\n",
            " [0.00225714 0.00313579 0.00379232 ... 0.00322311 0.00670722 0.0041651 ]\n",
            " [0.00148484 0.00108161 0.00218998 ... 0.00244326 0.00277678 0.00230441]\n",
            " ...\n",
            " [0.00205307 0.00197945 0.00226313 ... 0.00280221 0.00516975 0.00277255]\n",
            " [0.00219761 0.00223969 0.00344602 ... 0.00298263 0.00301326 0.00338268]\n",
            " [0.0019745  0.00252106 0.00259921 ... 0.00278114 0.0041468  0.00322847]]\n",
            "EPOCH: 0, train_loss: 0.1398304985998661\n",
            "EPOCH: 0, valid_loss: 0.01888701255972448\n",
            "updating best model on Fold=2\n",
            "EPOCH: 1, train_loss: 0.019024740710804528\n",
            "EPOCH: 1, valid_loss: 0.01809575521436177\n",
            "updating best model on Fold=2\n",
            "[[0.00370408 0.00506939 0.00462593 ... 0.00491823 0.00551017 0.00538496]\n",
            " [0.00307214 0.00478975 0.00541333 ... 0.0047762  0.00985738 0.0058361 ]\n",
            " [0.00237612 0.00168476 0.0035655  ... 0.00374868 0.00486623 0.00404452]\n",
            " ...\n",
            " [0.00318633 0.00287662 0.00333158 ... 0.00399217 0.00688114 0.00402369]\n",
            " [0.00332133 0.00351064 0.0053532  ... 0.00471768 0.00522469 0.00504253]\n",
            " [0.00275123 0.00354121 0.00358438 ... 0.00385227 0.00567101 0.00420006]]\n",
            "EPOCH: 0, train_loss: 0.14179905315223557\n",
            "EPOCH: 0, valid_loss: 0.018430242805104507\n",
            "updating best model on Fold=3\n",
            "EPOCH: 1, train_loss: 0.01890688665186199\n",
            "EPOCH: 1, valid_loss: 0.01760316027426406\n",
            "updating best model on Fold=3\n",
            "[[0.00493188 0.00687132 0.00612952 ... 0.00614077 0.00770475 0.00706601]\n",
            " [0.00406912 0.00657833 0.00828225 ... 0.00676274 0.01328207 0.00939639]\n",
            " [0.00306507 0.00215917 0.00471819 ... 0.00509683 0.00647736 0.00523013]\n",
            " ...\n",
            " [0.00439968 0.00388723 0.0046251  ... 0.00537848 0.00878447 0.00555143]\n",
            " [0.00480168 0.00479015 0.00754234 ... 0.00650633 0.00725088 0.00740909]\n",
            " [0.00361628 0.00485594 0.00486344 ... 0.00499454 0.0076587  0.00574714]]\n",
            "EPOCH: 0, train_loss: 0.13974863112022812\n",
            "EPOCH: 0, valid_loss: 0.01884181880833287\n",
            "updating best model on Fold=4\n",
            "EPOCH: 1, train_loss: 0.018818395418558745\n",
            "EPOCH: 1, valid_loss: 0.017841783989416927\n",
            "updating best model on Fold=4\n",
            "[[0.00609646 0.00847554 0.00728388 ... 0.00747756 0.0092283  0.00851407]\n",
            " [0.00486765 0.00841385 0.00967859 ... 0.00808115 0.01545311 0.01133433]\n",
            " [0.00373758 0.00273673 0.00554452 ... 0.00629889 0.00788662 0.00662379]\n",
            " ...\n",
            " [0.00545955 0.0048611  0.00561804 ... 0.00678244 0.01079805 0.00702817]\n",
            " [0.00580958 0.00640135 0.00890734 ... 0.00804035 0.00866938 0.00898254]\n",
            " [0.00464369 0.00638949 0.00599216 ... 0.00625686 0.00935469 0.00720014]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_g9DA4RYwgV"
      },
      "source": [
        "sample_submission = pd.read_csv('/content/drive/My Drive/moa/sample_submission.csv')"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GzfJHx8feli"
      },
      "source": [
        "y = pd.DataFrame(data=predictions)\n",
        "y.columns = target_cols"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQ3ChHg1YyUc"
      },
      "source": [
        "sub = sample_submission.drop(columns=target_cols)\n",
        "frames = [sub, y]\n",
        "sub = pd.concat(frames, axis=1)"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "LdBp-IL_W2sJ",
        "outputId": "d2174bfa-d328-42ea-af67-26b59284182e"
      },
      "source": [
        "submission = sub.to_csv(\"submission.csv\",index=False)\n",
        "teste = pd.read_csv(\"submission.csv\")\n",
        "teste"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sig_id</th>\n",
              "      <th>5-alpha_reductase_inhibitor</th>\n",
              "      <th>11-beta-hsd1_inhibitor</th>\n",
              "      <th>acat_inhibitor</th>\n",
              "      <th>acetylcholine_receptor_agonist</th>\n",
              "      <th>acetylcholine_receptor_antagonist</th>\n",
              "      <th>acetylcholinesterase_inhibitor</th>\n",
              "      <th>adenosine_receptor_agonist</th>\n",
              "      <th>adenosine_receptor_antagonist</th>\n",
              "      <th>adenylyl_cyclase_activator</th>\n",
              "      <th>adrenergic_receptor_agonist</th>\n",
              "      <th>adrenergic_receptor_antagonist</th>\n",
              "      <th>akt_inhibitor</th>\n",
              "      <th>aldehyde_dehydrogenase_inhibitor</th>\n",
              "      <th>alk_inhibitor</th>\n",
              "      <th>ampk_activator</th>\n",
              "      <th>analgesic</th>\n",
              "      <th>androgen_receptor_agonist</th>\n",
              "      <th>androgen_receptor_antagonist</th>\n",
              "      <th>anesthetic_-_local</th>\n",
              "      <th>angiogenesis_inhibitor</th>\n",
              "      <th>angiotensin_receptor_antagonist</th>\n",
              "      <th>anti-inflammatory</th>\n",
              "      <th>antiarrhythmic</th>\n",
              "      <th>antibiotic</th>\n",
              "      <th>anticonvulsant</th>\n",
              "      <th>antifungal</th>\n",
              "      <th>antihistamine</th>\n",
              "      <th>antimalarial</th>\n",
              "      <th>antioxidant</th>\n",
              "      <th>antiprotozoal</th>\n",
              "      <th>antiviral</th>\n",
              "      <th>apoptosis_stimulant</th>\n",
              "      <th>aromatase_inhibitor</th>\n",
              "      <th>atm_kinase_inhibitor</th>\n",
              "      <th>atp-sensitive_potassium_channel_antagonist</th>\n",
              "      <th>atp_synthase_inhibitor</th>\n",
              "      <th>atpase_inhibitor</th>\n",
              "      <th>atr_kinase_inhibitor</th>\n",
              "      <th>aurora_kinase_inhibitor</th>\n",
              "      <th>...</th>\n",
              "      <th>protein_synthesis_inhibitor</th>\n",
              "      <th>protein_tyrosine_kinase_inhibitor</th>\n",
              "      <th>radiopaque_medium</th>\n",
              "      <th>raf_inhibitor</th>\n",
              "      <th>ras_gtpase_inhibitor</th>\n",
              "      <th>retinoid_receptor_agonist</th>\n",
              "      <th>retinoid_receptor_antagonist</th>\n",
              "      <th>rho_associated_kinase_inhibitor</th>\n",
              "      <th>ribonucleoside_reductase_inhibitor</th>\n",
              "      <th>rna_polymerase_inhibitor</th>\n",
              "      <th>serotonin_receptor_agonist</th>\n",
              "      <th>serotonin_receptor_antagonist</th>\n",
              "      <th>serotonin_reuptake_inhibitor</th>\n",
              "      <th>sigma_receptor_agonist</th>\n",
              "      <th>sigma_receptor_antagonist</th>\n",
              "      <th>smoothened_receptor_antagonist</th>\n",
              "      <th>sodium_channel_inhibitor</th>\n",
              "      <th>sphingosine_receptor_agonist</th>\n",
              "      <th>src_inhibitor</th>\n",
              "      <th>steroid</th>\n",
              "      <th>syk_inhibitor</th>\n",
              "      <th>tachykinin_antagonist</th>\n",
              "      <th>tgf-beta_receptor_inhibitor</th>\n",
              "      <th>thrombin_inhibitor</th>\n",
              "      <th>thymidylate_synthase_inhibitor</th>\n",
              "      <th>tlr_agonist</th>\n",
              "      <th>tlr_antagonist</th>\n",
              "      <th>tnf_inhibitor</th>\n",
              "      <th>topoisomerase_inhibitor</th>\n",
              "      <th>transient_receptor_potential_channel_antagonist</th>\n",
              "      <th>tropomyosin_receptor_kinase_inhibitor</th>\n",
              "      <th>trpv_agonist</th>\n",
              "      <th>trpv_antagonist</th>\n",
              "      <th>tubulin_inhibitor</th>\n",
              "      <th>tyrosine_kinase_inhibitor</th>\n",
              "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
              "      <th>vegfr_inhibitor</th>\n",
              "      <th>vitamin_b</th>\n",
              "      <th>vitamin_d_receptor_agonist</th>\n",
              "      <th>wnt_inhibitor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id_0004d9e33</td>\n",
              "      <td>0.001219</td>\n",
              "      <td>0.001695</td>\n",
              "      <td>0.001457</td>\n",
              "      <td>0.010081</td>\n",
              "      <td>0.017208</td>\n",
              "      <td>0.003812</td>\n",
              "      <td>0.002288</td>\n",
              "      <td>0.005983</td>\n",
              "      <td>0.001107</td>\n",
              "      <td>0.014914</td>\n",
              "      <td>0.018806</td>\n",
              "      <td>0.003097</td>\n",
              "      <td>0.001024</td>\n",
              "      <td>0.002245</td>\n",
              "      <td>0.000989</td>\n",
              "      <td>0.000971</td>\n",
              "      <td>0.002484</td>\n",
              "      <td>0.004664</td>\n",
              "      <td>0.005051</td>\n",
              "      <td>0.001791</td>\n",
              "      <td>0.002332</td>\n",
              "      <td>0.002554</td>\n",
              "      <td>0.001106</td>\n",
              "      <td>0.001438</td>\n",
              "      <td>0.001253</td>\n",
              "      <td>0.001177</td>\n",
              "      <td>0.001121</td>\n",
              "      <td>0.001223</td>\n",
              "      <td>0.003205</td>\n",
              "      <td>0.001935</td>\n",
              "      <td>0.001566</td>\n",
              "      <td>0.002354</td>\n",
              "      <td>0.002188</td>\n",
              "      <td>0.000978</td>\n",
              "      <td>0.000921</td>\n",
              "      <td>0.001142</td>\n",
              "      <td>0.004203</td>\n",
              "      <td>0.001362</td>\n",
              "      <td>0.003603</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003469</td>\n",
              "      <td>0.001257</td>\n",
              "      <td>0.003102</td>\n",
              "      <td>0.001050</td>\n",
              "      <td>0.001061</td>\n",
              "      <td>0.003556</td>\n",
              "      <td>0.000967</td>\n",
              "      <td>0.001854</td>\n",
              "      <td>0.001325</td>\n",
              "      <td>0.001232</td>\n",
              "      <td>0.011406</td>\n",
              "      <td>0.020664</td>\n",
              "      <td>0.002095</td>\n",
              "      <td>0.001805</td>\n",
              "      <td>0.001942</td>\n",
              "      <td>0.001091</td>\n",
              "      <td>0.016058</td>\n",
              "      <td>0.001321</td>\n",
              "      <td>0.002330</td>\n",
              "      <td>0.000886</td>\n",
              "      <td>0.001280</td>\n",
              "      <td>0.003147</td>\n",
              "      <td>0.001443</td>\n",
              "      <td>0.001505</td>\n",
              "      <td>0.001790</td>\n",
              "      <td>0.001681</td>\n",
              "      <td>0.000971</td>\n",
              "      <td>0.001750</td>\n",
              "      <td>0.003217</td>\n",
              "      <td>0.001405</td>\n",
              "      <td>0.001007</td>\n",
              "      <td>0.001614</td>\n",
              "      <td>0.002768</td>\n",
              "      <td>0.006160</td>\n",
              "      <td>0.003242</td>\n",
              "      <td>0.001003</td>\n",
              "      <td>0.008424</td>\n",
              "      <td>0.001496</td>\n",
              "      <td>0.001846</td>\n",
              "      <td>0.001703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id_001897cda</td>\n",
              "      <td>0.000974</td>\n",
              "      <td>0.001683</td>\n",
              "      <td>0.001936</td>\n",
              "      <td>0.007444</td>\n",
              "      <td>0.011880</td>\n",
              "      <td>0.005587</td>\n",
              "      <td>0.002342</td>\n",
              "      <td>0.007115</td>\n",
              "      <td>0.001501</td>\n",
              "      <td>0.014217</td>\n",
              "      <td>0.015911</td>\n",
              "      <td>0.004770</td>\n",
              "      <td>0.001484</td>\n",
              "      <td>0.002840</td>\n",
              "      <td>0.001507</td>\n",
              "      <td>0.001389</td>\n",
              "      <td>0.002333</td>\n",
              "      <td>0.004168</td>\n",
              "      <td>0.005180</td>\n",
              "      <td>0.002476</td>\n",
              "      <td>0.002219</td>\n",
              "      <td>0.002644</td>\n",
              "      <td>0.001428</td>\n",
              "      <td>0.002124</td>\n",
              "      <td>0.001523</td>\n",
              "      <td>0.001517</td>\n",
              "      <td>0.001368</td>\n",
              "      <td>0.002306</td>\n",
              "      <td>0.003861</td>\n",
              "      <td>0.002703</td>\n",
              "      <td>0.001526</td>\n",
              "      <td>0.003180</td>\n",
              "      <td>0.002271</td>\n",
              "      <td>0.001567</td>\n",
              "      <td>0.001327</td>\n",
              "      <td>0.002068</td>\n",
              "      <td>0.006619</td>\n",
              "      <td>0.002292</td>\n",
              "      <td>0.008526</td>\n",
              "      <td>...</td>\n",
              "      <td>0.004899</td>\n",
              "      <td>0.002259</td>\n",
              "      <td>0.002617</td>\n",
              "      <td>0.004366</td>\n",
              "      <td>0.001645</td>\n",
              "      <td>0.004023</td>\n",
              "      <td>0.001449</td>\n",
              "      <td>0.002131</td>\n",
              "      <td>0.003393</td>\n",
              "      <td>0.001736</td>\n",
              "      <td>0.011377</td>\n",
              "      <td>0.025690</td>\n",
              "      <td>0.002300</td>\n",
              "      <td>0.001972</td>\n",
              "      <td>0.002842</td>\n",
              "      <td>0.001558</td>\n",
              "      <td>0.013640</td>\n",
              "      <td>0.001516</td>\n",
              "      <td>0.004545</td>\n",
              "      <td>0.001303</td>\n",
              "      <td>0.001787</td>\n",
              "      <td>0.002416</td>\n",
              "      <td>0.001227</td>\n",
              "      <td>0.001773</td>\n",
              "      <td>0.001831</td>\n",
              "      <td>0.001693</td>\n",
              "      <td>0.001499</td>\n",
              "      <td>0.002640</td>\n",
              "      <td>0.006517</td>\n",
              "      <td>0.001744</td>\n",
              "      <td>0.001399</td>\n",
              "      <td>0.002510</td>\n",
              "      <td>0.002687</td>\n",
              "      <td>0.010270</td>\n",
              "      <td>0.006757</td>\n",
              "      <td>0.001415</td>\n",
              "      <td>0.014903</td>\n",
              "      <td>0.001616</td>\n",
              "      <td>0.003091</td>\n",
              "      <td>0.002267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id_002429b5b</td>\n",
              "      <td>0.000748</td>\n",
              "      <td>0.000547</td>\n",
              "      <td>0.001109</td>\n",
              "      <td>0.007365</td>\n",
              "      <td>0.008920</td>\n",
              "      <td>0.001894</td>\n",
              "      <td>0.002201</td>\n",
              "      <td>0.002065</td>\n",
              "      <td>0.000670</td>\n",
              "      <td>0.008350</td>\n",
              "      <td>0.012754</td>\n",
              "      <td>0.000741</td>\n",
              "      <td>0.000553</td>\n",
              "      <td>0.000850</td>\n",
              "      <td>0.000880</td>\n",
              "      <td>0.000964</td>\n",
              "      <td>0.001220</td>\n",
              "      <td>0.002591</td>\n",
              "      <td>0.001287</td>\n",
              "      <td>0.001436</td>\n",
              "      <td>0.001215</td>\n",
              "      <td>0.003253</td>\n",
              "      <td>0.000570</td>\n",
              "      <td>0.001792</td>\n",
              "      <td>0.000628</td>\n",
              "      <td>0.000685</td>\n",
              "      <td>0.000679</td>\n",
              "      <td>0.000776</td>\n",
              "      <td>0.002758</td>\n",
              "      <td>0.001446</td>\n",
              "      <td>0.000873</td>\n",
              "      <td>0.001769</td>\n",
              "      <td>0.002147</td>\n",
              "      <td>0.000619</td>\n",
              "      <td>0.000636</td>\n",
              "      <td>0.000575</td>\n",
              "      <td>0.002123</td>\n",
              "      <td>0.000569</td>\n",
              "      <td>0.000896</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003135</td>\n",
              "      <td>0.001007</td>\n",
              "      <td>0.001566</td>\n",
              "      <td>0.002918</td>\n",
              "      <td>0.000720</td>\n",
              "      <td>0.001357</td>\n",
              "      <td>0.000695</td>\n",
              "      <td>0.001061</td>\n",
              "      <td>0.001364</td>\n",
              "      <td>0.001066</td>\n",
              "      <td>0.007775</td>\n",
              "      <td>0.008841</td>\n",
              "      <td>0.001686</td>\n",
              "      <td>0.001025</td>\n",
              "      <td>0.000890</td>\n",
              "      <td>0.001520</td>\n",
              "      <td>0.006072</td>\n",
              "      <td>0.001312</td>\n",
              "      <td>0.001352</td>\n",
              "      <td>0.000830</td>\n",
              "      <td>0.000646</td>\n",
              "      <td>0.001189</td>\n",
              "      <td>0.001455</td>\n",
              "      <td>0.000680</td>\n",
              "      <td>0.001097</td>\n",
              "      <td>0.001011</td>\n",
              "      <td>0.000828</td>\n",
              "      <td>0.001058</td>\n",
              "      <td>0.002133</td>\n",
              "      <td>0.000652</td>\n",
              "      <td>0.000717</td>\n",
              "      <td>0.000650</td>\n",
              "      <td>0.001562</td>\n",
              "      <td>0.004057</td>\n",
              "      <td>0.001425</td>\n",
              "      <td>0.000612</td>\n",
              "      <td>0.002203</td>\n",
              "      <td>0.001260</td>\n",
              "      <td>0.001577</td>\n",
              "      <td>0.001325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>id_00276f245</td>\n",
              "      <td>0.000725</td>\n",
              "      <td>0.000995</td>\n",
              "      <td>0.001426</td>\n",
              "      <td>0.006804</td>\n",
              "      <td>0.008114</td>\n",
              "      <td>0.003143</td>\n",
              "      <td>0.001670</td>\n",
              "      <td>0.003749</td>\n",
              "      <td>0.001024</td>\n",
              "      <td>0.009819</td>\n",
              "      <td>0.012396</td>\n",
              "      <td>0.002346</td>\n",
              "      <td>0.000923</td>\n",
              "      <td>0.002036</td>\n",
              "      <td>0.001002</td>\n",
              "      <td>0.001094</td>\n",
              "      <td>0.001331</td>\n",
              "      <td>0.003152</td>\n",
              "      <td>0.002418</td>\n",
              "      <td>0.001591</td>\n",
              "      <td>0.001551</td>\n",
              "      <td>0.002021</td>\n",
              "      <td>0.000923</td>\n",
              "      <td>0.001487</td>\n",
              "      <td>0.000930</td>\n",
              "      <td>0.000971</td>\n",
              "      <td>0.000816</td>\n",
              "      <td>0.001433</td>\n",
              "      <td>0.002511</td>\n",
              "      <td>0.001760</td>\n",
              "      <td>0.000851</td>\n",
              "      <td>0.002063</td>\n",
              "      <td>0.001561</td>\n",
              "      <td>0.000963</td>\n",
              "      <td>0.000856</td>\n",
              "      <td>0.001250</td>\n",
              "      <td>0.004313</td>\n",
              "      <td>0.001279</td>\n",
              "      <td>0.004168</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003582</td>\n",
              "      <td>0.001456</td>\n",
              "      <td>0.001830</td>\n",
              "      <td>0.004450</td>\n",
              "      <td>0.001032</td>\n",
              "      <td>0.002376</td>\n",
              "      <td>0.000956</td>\n",
              "      <td>0.001454</td>\n",
              "      <td>0.002049</td>\n",
              "      <td>0.000912</td>\n",
              "      <td>0.007084</td>\n",
              "      <td>0.015397</td>\n",
              "      <td>0.001846</td>\n",
              "      <td>0.001086</td>\n",
              "      <td>0.001601</td>\n",
              "      <td>0.001213</td>\n",
              "      <td>0.008665</td>\n",
              "      <td>0.001075</td>\n",
              "      <td>0.002888</td>\n",
              "      <td>0.000916</td>\n",
              "      <td>0.001106</td>\n",
              "      <td>0.001611</td>\n",
              "      <td>0.000874</td>\n",
              "      <td>0.001114</td>\n",
              "      <td>0.001364</td>\n",
              "      <td>0.001069</td>\n",
              "      <td>0.000997</td>\n",
              "      <td>0.001416</td>\n",
              "      <td>0.003838</td>\n",
              "      <td>0.000960</td>\n",
              "      <td>0.000862</td>\n",
              "      <td>0.001282</td>\n",
              "      <td>0.002007</td>\n",
              "      <td>0.009754</td>\n",
              "      <td>0.003840</td>\n",
              "      <td>0.000870</td>\n",
              "      <td>0.009858</td>\n",
              "      <td>0.001150</td>\n",
              "      <td>0.002006</td>\n",
              "      <td>0.001719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id_0027f1083</td>\n",
              "      <td>0.001435</td>\n",
              "      <td>0.000830</td>\n",
              "      <td>0.001231</td>\n",
              "      <td>0.008723</td>\n",
              "      <td>0.012680</td>\n",
              "      <td>0.002166</td>\n",
              "      <td>0.002730</td>\n",
              "      <td>0.002622</td>\n",
              "      <td>0.000801</td>\n",
              "      <td>0.011267</td>\n",
              "      <td>0.018331</td>\n",
              "      <td>0.001178</td>\n",
              "      <td>0.000749</td>\n",
              "      <td>0.001086</td>\n",
              "      <td>0.001100</td>\n",
              "      <td>0.000947</td>\n",
              "      <td>0.001870</td>\n",
              "      <td>0.003642</td>\n",
              "      <td>0.002194</td>\n",
              "      <td>0.001786</td>\n",
              "      <td>0.001647</td>\n",
              "      <td>0.004360</td>\n",
              "      <td>0.000701</td>\n",
              "      <td>0.001933</td>\n",
              "      <td>0.000853</td>\n",
              "      <td>0.000839</td>\n",
              "      <td>0.001028</td>\n",
              "      <td>0.000747</td>\n",
              "      <td>0.002982</td>\n",
              "      <td>0.001738</td>\n",
              "      <td>0.001552</td>\n",
              "      <td>0.002205</td>\n",
              "      <td>0.002786</td>\n",
              "      <td>0.000745</td>\n",
              "      <td>0.000764</td>\n",
              "      <td>0.000682</td>\n",
              "      <td>0.002515</td>\n",
              "      <td>0.000741</td>\n",
              "      <td>0.001277</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003819</td>\n",
              "      <td>0.001072</td>\n",
              "      <td>0.002504</td>\n",
              "      <td>0.002190</td>\n",
              "      <td>0.000890</td>\n",
              "      <td>0.001933</td>\n",
              "      <td>0.000801</td>\n",
              "      <td>0.001661</td>\n",
              "      <td>0.001326</td>\n",
              "      <td>0.001789</td>\n",
              "      <td>0.011163</td>\n",
              "      <td>0.012132</td>\n",
              "      <td>0.002002</td>\n",
              "      <td>0.001836</td>\n",
              "      <td>0.001212</td>\n",
              "      <td>0.001747</td>\n",
              "      <td>0.010434</td>\n",
              "      <td>0.001711</td>\n",
              "      <td>0.001885</td>\n",
              "      <td>0.000918</td>\n",
              "      <td>0.000867</td>\n",
              "      <td>0.001944</td>\n",
              "      <td>0.002314</td>\n",
              "      <td>0.000888</td>\n",
              "      <td>0.001655</td>\n",
              "      <td>0.001792</td>\n",
              "      <td>0.000869</td>\n",
              "      <td>0.001632</td>\n",
              "      <td>0.002953</td>\n",
              "      <td>0.000938</td>\n",
              "      <td>0.000951</td>\n",
              "      <td>0.001030</td>\n",
              "      <td>0.001874</td>\n",
              "      <td>0.003957</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.000775</td>\n",
              "      <td>0.002210</td>\n",
              "      <td>0.001639</td>\n",
              "      <td>0.001589</td>\n",
              "      <td>0.001398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3977</th>\n",
              "      <td>id_ff7004b87</td>\n",
              "      <td>0.000815</td>\n",
              "      <td>0.001745</td>\n",
              "      <td>0.001424</td>\n",
              "      <td>0.006447</td>\n",
              "      <td>0.009276</td>\n",
              "      <td>0.003688</td>\n",
              "      <td>0.001267</td>\n",
              "      <td>0.005380</td>\n",
              "      <td>0.001265</td>\n",
              "      <td>0.011393</td>\n",
              "      <td>0.011630</td>\n",
              "      <td>0.007181</td>\n",
              "      <td>0.001334</td>\n",
              "      <td>0.003926</td>\n",
              "      <td>0.000869</td>\n",
              "      <td>0.000896</td>\n",
              "      <td>0.001613</td>\n",
              "      <td>0.003623</td>\n",
              "      <td>0.004110</td>\n",
              "      <td>0.001369</td>\n",
              "      <td>0.002030</td>\n",
              "      <td>0.001310</td>\n",
              "      <td>0.001218</td>\n",
              "      <td>0.001114</td>\n",
              "      <td>0.001180</td>\n",
              "      <td>0.001097</td>\n",
              "      <td>0.000974</td>\n",
              "      <td>0.001562</td>\n",
              "      <td>0.001844</td>\n",
              "      <td>0.001515</td>\n",
              "      <td>0.000999</td>\n",
              "      <td>0.002062</td>\n",
              "      <td>0.001321</td>\n",
              "      <td>0.001099</td>\n",
              "      <td>0.000883</td>\n",
              "      <td>0.002112</td>\n",
              "      <td>0.005990</td>\n",
              "      <td>0.002417</td>\n",
              "      <td>0.011279</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003186</td>\n",
              "      <td>0.001195</td>\n",
              "      <td>0.002451</td>\n",
              "      <td>0.001981</td>\n",
              "      <td>0.001148</td>\n",
              "      <td>0.003723</td>\n",
              "      <td>0.000979</td>\n",
              "      <td>0.001716</td>\n",
              "      <td>0.001926</td>\n",
              "      <td>0.000789</td>\n",
              "      <td>0.007125</td>\n",
              "      <td>0.017520</td>\n",
              "      <td>0.001635</td>\n",
              "      <td>0.001273</td>\n",
              "      <td>0.001959</td>\n",
              "      <td>0.000989</td>\n",
              "      <td>0.014155</td>\n",
              "      <td>0.000902</td>\n",
              "      <td>0.004638</td>\n",
              "      <td>0.000751</td>\n",
              "      <td>0.001644</td>\n",
              "      <td>0.002647</td>\n",
              "      <td>0.000715</td>\n",
              "      <td>0.001536</td>\n",
              "      <td>0.001618</td>\n",
              "      <td>0.001215</td>\n",
              "      <td>0.000896</td>\n",
              "      <td>0.001714</td>\n",
              "      <td>0.004462</td>\n",
              "      <td>0.001286</td>\n",
              "      <td>0.000915</td>\n",
              "      <td>0.002308</td>\n",
              "      <td>0.002448</td>\n",
              "      <td>0.015452</td>\n",
              "      <td>0.006116</td>\n",
              "      <td>0.000967</td>\n",
              "      <td>0.022595</td>\n",
              "      <td>0.001141</td>\n",
              "      <td>0.001660</td>\n",
              "      <td>0.001615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3978</th>\n",
              "      <td>id_ff925dd0d</td>\n",
              "      <td>0.001803</td>\n",
              "      <td>0.001192</td>\n",
              "      <td>0.001258</td>\n",
              "      <td>0.008481</td>\n",
              "      <td>0.019224</td>\n",
              "      <td>0.003062</td>\n",
              "      <td>0.003143</td>\n",
              "      <td>0.003657</td>\n",
              "      <td>0.000868</td>\n",
              "      <td>0.015002</td>\n",
              "      <td>0.021788</td>\n",
              "      <td>0.001463</td>\n",
              "      <td>0.000801</td>\n",
              "      <td>0.001213</td>\n",
              "      <td>0.001116</td>\n",
              "      <td>0.000955</td>\n",
              "      <td>0.002959</td>\n",
              "      <td>0.004695</td>\n",
              "      <td>0.004012</td>\n",
              "      <td>0.002022</td>\n",
              "      <td>0.001790</td>\n",
              "      <td>0.004815</td>\n",
              "      <td>0.000826</td>\n",
              "      <td>0.002026</td>\n",
              "      <td>0.001046</td>\n",
              "      <td>0.001053</td>\n",
              "      <td>0.001103</td>\n",
              "      <td>0.000852</td>\n",
              "      <td>0.004080</td>\n",
              "      <td>0.002156</td>\n",
              "      <td>0.002027</td>\n",
              "      <td>0.002428</td>\n",
              "      <td>0.003161</td>\n",
              "      <td>0.000817</td>\n",
              "      <td>0.000875</td>\n",
              "      <td>0.000739</td>\n",
              "      <td>0.002997</td>\n",
              "      <td>0.000812</td>\n",
              "      <td>0.001417</td>\n",
              "      <td>...</td>\n",
              "      <td>0.004138</td>\n",
              "      <td>0.001227</td>\n",
              "      <td>0.003002</td>\n",
              "      <td>0.001513</td>\n",
              "      <td>0.000935</td>\n",
              "      <td>0.002607</td>\n",
              "      <td>0.000850</td>\n",
              "      <td>0.001641</td>\n",
              "      <td>0.001298</td>\n",
              "      <td>0.002020</td>\n",
              "      <td>0.013552</td>\n",
              "      <td>0.018315</td>\n",
              "      <td>0.002149</td>\n",
              "      <td>0.002224</td>\n",
              "      <td>0.001561</td>\n",
              "      <td>0.001432</td>\n",
              "      <td>0.013908</td>\n",
              "      <td>0.001640</td>\n",
              "      <td>0.002033</td>\n",
              "      <td>0.000937</td>\n",
              "      <td>0.000990</td>\n",
              "      <td>0.002795</td>\n",
              "      <td>0.002601</td>\n",
              "      <td>0.001112</td>\n",
              "      <td>0.001726</td>\n",
              "      <td>0.002153</td>\n",
              "      <td>0.000936</td>\n",
              "      <td>0.001768</td>\n",
              "      <td>0.003182</td>\n",
              "      <td>0.001202</td>\n",
              "      <td>0.000946</td>\n",
              "      <td>0.001201</td>\n",
              "      <td>0.002005</td>\n",
              "      <td>0.003254</td>\n",
              "      <td>0.001854</td>\n",
              "      <td>0.000875</td>\n",
              "      <td>0.002650</td>\n",
              "      <td>0.001609</td>\n",
              "      <td>0.001719</td>\n",
              "      <td>0.001400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3979</th>\n",
              "      <td>id_ffb710450</td>\n",
              "      <td>0.001092</td>\n",
              "      <td>0.000972</td>\n",
              "      <td>0.001124</td>\n",
              "      <td>0.006341</td>\n",
              "      <td>0.017261</td>\n",
              "      <td>0.004056</td>\n",
              "      <td>0.002616</td>\n",
              "      <td>0.004635</td>\n",
              "      <td>0.000806</td>\n",
              "      <td>0.013957</td>\n",
              "      <td>0.019750</td>\n",
              "      <td>0.001414</td>\n",
              "      <td>0.000791</td>\n",
              "      <td>0.001096</td>\n",
              "      <td>0.001175</td>\n",
              "      <td>0.000875</td>\n",
              "      <td>0.002324</td>\n",
              "      <td>0.003896</td>\n",
              "      <td>0.004249</td>\n",
              "      <td>0.001982</td>\n",
              "      <td>0.001353</td>\n",
              "      <td>0.003988</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.001788</td>\n",
              "      <td>0.000946</td>\n",
              "      <td>0.000950</td>\n",
              "      <td>0.000971</td>\n",
              "      <td>0.001082</td>\n",
              "      <td>0.004056</td>\n",
              "      <td>0.002393</td>\n",
              "      <td>0.001571</td>\n",
              "      <td>0.002522</td>\n",
              "      <td>0.002697</td>\n",
              "      <td>0.000864</td>\n",
              "      <td>0.000845</td>\n",
              "      <td>0.000827</td>\n",
              "      <td>0.003673</td>\n",
              "      <td>0.000853</td>\n",
              "      <td>0.001796</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003926</td>\n",
              "      <td>0.001505</td>\n",
              "      <td>0.002216</td>\n",
              "      <td>0.002331</td>\n",
              "      <td>0.000957</td>\n",
              "      <td>0.002582</td>\n",
              "      <td>0.000873</td>\n",
              "      <td>0.001501</td>\n",
              "      <td>0.001692</td>\n",
              "      <td>0.001894</td>\n",
              "      <td>0.013088</td>\n",
              "      <td>0.025715</td>\n",
              "      <td>0.001928</td>\n",
              "      <td>0.001830</td>\n",
              "      <td>0.001672</td>\n",
              "      <td>0.001193</td>\n",
              "      <td>0.012410</td>\n",
              "      <td>0.001427</td>\n",
              "      <td>0.002033</td>\n",
              "      <td>0.000940</td>\n",
              "      <td>0.000894</td>\n",
              "      <td>0.001998</td>\n",
              "      <td>0.001465</td>\n",
              "      <td>0.001051</td>\n",
              "      <td>0.001343</td>\n",
              "      <td>0.001663</td>\n",
              "      <td>0.000991</td>\n",
              "      <td>0.002182</td>\n",
              "      <td>0.003740</td>\n",
              "      <td>0.001117</td>\n",
              "      <td>0.000941</td>\n",
              "      <td>0.001306</td>\n",
              "      <td>0.001633</td>\n",
              "      <td>0.003430</td>\n",
              "      <td>0.002462</td>\n",
              "      <td>0.000859</td>\n",
              "      <td>0.003119</td>\n",
              "      <td>0.001356</td>\n",
              "      <td>0.002160</td>\n",
              "      <td>0.001406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3980</th>\n",
              "      <td>id_ffbb869f2</td>\n",
              "      <td>0.001162</td>\n",
              "      <td>0.001280</td>\n",
              "      <td>0.001781</td>\n",
              "      <td>0.009510</td>\n",
              "      <td>0.017216</td>\n",
              "      <td>0.002831</td>\n",
              "      <td>0.003254</td>\n",
              "      <td>0.002749</td>\n",
              "      <td>0.001156</td>\n",
              "      <td>0.007804</td>\n",
              "      <td>0.013928</td>\n",
              "      <td>0.001353</td>\n",
              "      <td>0.000891</td>\n",
              "      <td>0.002108</td>\n",
              "      <td>0.000864</td>\n",
              "      <td>0.001920</td>\n",
              "      <td>0.002970</td>\n",
              "      <td>0.004673</td>\n",
              "      <td>0.001757</td>\n",
              "      <td>0.001263</td>\n",
              "      <td>0.002635</td>\n",
              "      <td>0.003526</td>\n",
              "      <td>0.000964</td>\n",
              "      <td>0.002678</td>\n",
              "      <td>0.001216</td>\n",
              "      <td>0.001561</td>\n",
              "      <td>0.000778</td>\n",
              "      <td>0.001273</td>\n",
              "      <td>0.003163</td>\n",
              "      <td>0.001547</td>\n",
              "      <td>0.001195</td>\n",
              "      <td>0.001792</td>\n",
              "      <td>0.002676</td>\n",
              "      <td>0.000924</td>\n",
              "      <td>0.000940</td>\n",
              "      <td>0.000816</td>\n",
              "      <td>0.002510</td>\n",
              "      <td>0.001008</td>\n",
              "      <td>0.000825</td>\n",
              "      <td>...</td>\n",
              "      <td>0.004446</td>\n",
              "      <td>0.001279</td>\n",
              "      <td>0.002209</td>\n",
              "      <td>0.003667</td>\n",
              "      <td>0.000810</td>\n",
              "      <td>0.001733</td>\n",
              "      <td>0.000903</td>\n",
              "      <td>0.001093</td>\n",
              "      <td>0.001335</td>\n",
              "      <td>0.000891</td>\n",
              "      <td>0.009533</td>\n",
              "      <td>0.014655</td>\n",
              "      <td>0.001588</td>\n",
              "      <td>0.000882</td>\n",
              "      <td>0.001813</td>\n",
              "      <td>0.001783</td>\n",
              "      <td>0.004531</td>\n",
              "      <td>0.001243</td>\n",
              "      <td>0.002319</td>\n",
              "      <td>0.001140</td>\n",
              "      <td>0.001274</td>\n",
              "      <td>0.002868</td>\n",
              "      <td>0.001977</td>\n",
              "      <td>0.001415</td>\n",
              "      <td>0.002253</td>\n",
              "      <td>0.001261</td>\n",
              "      <td>0.001077</td>\n",
              "      <td>0.000673</td>\n",
              "      <td>0.002789</td>\n",
              "      <td>0.001086</td>\n",
              "      <td>0.000818</td>\n",
              "      <td>0.000852</td>\n",
              "      <td>0.002749</td>\n",
              "      <td>0.004810</td>\n",
              "      <td>0.003416</td>\n",
              "      <td>0.000850</td>\n",
              "      <td>0.005649</td>\n",
              "      <td>0.001608</td>\n",
              "      <td>0.001734</td>\n",
              "      <td>0.001797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3981</th>\n",
              "      <td>id_ffd5800b6</td>\n",
              "      <td>0.000929</td>\n",
              "      <td>0.001278</td>\n",
              "      <td>0.001198</td>\n",
              "      <td>0.008166</td>\n",
              "      <td>0.016742</td>\n",
              "      <td>0.004139</td>\n",
              "      <td>0.002220</td>\n",
              "      <td>0.005962</td>\n",
              "      <td>0.000889</td>\n",
              "      <td>0.014985</td>\n",
              "      <td>0.018906</td>\n",
              "      <td>0.002399</td>\n",
              "      <td>0.000846</td>\n",
              "      <td>0.001563</td>\n",
              "      <td>0.000991</td>\n",
              "      <td>0.000839</td>\n",
              "      <td>0.002134</td>\n",
              "      <td>0.004061</td>\n",
              "      <td>0.004965</td>\n",
              "      <td>0.001874</td>\n",
              "      <td>0.001613</td>\n",
              "      <td>0.002669</td>\n",
              "      <td>0.000917</td>\n",
              "      <td>0.001359</td>\n",
              "      <td>0.001010</td>\n",
              "      <td>0.000984</td>\n",
              "      <td>0.000990</td>\n",
              "      <td>0.001145</td>\n",
              "      <td>0.003640</td>\n",
              "      <td>0.002116</td>\n",
              "      <td>0.001400</td>\n",
              "      <td>0.002360</td>\n",
              "      <td>0.002103</td>\n",
              "      <td>0.000856</td>\n",
              "      <td>0.000805</td>\n",
              "      <td>0.000923</td>\n",
              "      <td>0.003977</td>\n",
              "      <td>0.001103</td>\n",
              "      <td>0.003144</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003071</td>\n",
              "      <td>0.001277</td>\n",
              "      <td>0.002580</td>\n",
              "      <td>0.001120</td>\n",
              "      <td>0.001007</td>\n",
              "      <td>0.003120</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.001499</td>\n",
              "      <td>0.001423</td>\n",
              "      <td>0.001323</td>\n",
              "      <td>0.011020</td>\n",
              "      <td>0.021707</td>\n",
              "      <td>0.001902</td>\n",
              "      <td>0.001724</td>\n",
              "      <td>0.001742</td>\n",
              "      <td>0.000967</td>\n",
              "      <td>0.016842</td>\n",
              "      <td>0.001174</td>\n",
              "      <td>0.002120</td>\n",
              "      <td>0.000774</td>\n",
              "      <td>0.001031</td>\n",
              "      <td>0.002201</td>\n",
              "      <td>0.001169</td>\n",
              "      <td>0.001208</td>\n",
              "      <td>0.001298</td>\n",
              "      <td>0.001453</td>\n",
              "      <td>0.000909</td>\n",
              "      <td>0.002003</td>\n",
              "      <td>0.003047</td>\n",
              "      <td>0.001209</td>\n",
              "      <td>0.000915</td>\n",
              "      <td>0.001518</td>\n",
              "      <td>0.002152</td>\n",
              "      <td>0.005286</td>\n",
              "      <td>0.002743</td>\n",
              "      <td>0.000877</td>\n",
              "      <td>0.006752</td>\n",
              "      <td>0.001251</td>\n",
              "      <td>0.001871</td>\n",
              "      <td>0.001440</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3982 rows Ã— 207 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            sig_id  ...  wnt_inhibitor\n",
              "0     id_0004d9e33  ...       0.001703\n",
              "1     id_001897cda  ...       0.002267\n",
              "2     id_002429b5b  ...       0.001325\n",
              "3     id_00276f245  ...       0.001719\n",
              "4     id_0027f1083  ...       0.001398\n",
              "...            ...  ...            ...\n",
              "3977  id_ff7004b87  ...       0.001615\n",
              "3978  id_ff925dd0d  ...       0.001400\n",
              "3979  id_ffb710450  ...       0.001406\n",
              "3980  id_ffbb869f2  ...       0.001797\n",
              "3981  id_ffd5800b6  ...       0.001440\n",
              "\n",
              "[3982 rows x 207 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "moa-chall-train-colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "YO_rhpWK7jtP"
      },
      "source": [
        "# # This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# # For example, here's several helpful packages to load\n",
        "\n",
        "# import numpy as np # linear algebra\n",
        "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# # Input data files are available in the read-only \"../input/\" directory\n",
        "# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "# import os\n",
        "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "#     for filename in filenames:\n",
        "#         print(os.path.join(dirname, filename))\n",
        "\n",
        "# # You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "Xq_3iUWH7jtU"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "NDLO-u0X7jtY"
      },
      "source": [
        "train_features = pd.read_csv('/content/drive/My Drive/moa/train_features.csv')\n",
        "train_targets_scored = pd.read_csv('/content/drive/My Drive/moa/train_targets_scored.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lXuhUUG8wgh"
      },
      "source": [
        "train_features.dtypes.sort_values()\n",
        "\n",
        "X = train_features.iloc[:, 1:].values\n",
        "y = train_targets_scored.iloc[:, 1:].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAP05kHG820B"
      },
      "source": [
        "# oneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
        "X = np.array(ct.fit_transform(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UblZ8Ce885o5",
        "outputId": "7cac6d84-6421-4402-e7c2-6845298e9891",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X[:, 0:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.0, 1.0, 24, 'D1', 1.062],\n",
              "       [0.0, 1.0, 72, 'D1', 0.0743],\n",
              "       [0.0, 1.0, 48, 'D1', 0.628],\n",
              "       ...,\n",
              "       [1.0, 0.0, 48, 'D2', 0.3942],\n",
              "       [0.0, 1.0, 24, 'D1', 0.666],\n",
              "       [0.0, 1.0, 72, 'D1', -0.8598]], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbIYQPTu84DZ"
      },
      "source": [
        "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [3])], remainder='passthrough')\n",
        "X = np.array(ct.fit_transform(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l39TjDEa87u5",
        "outputId": "65b2ec9f-5dbe-48b2-bfe8-2a253525bcb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X[:, 0:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.0, 0.0, 0.0, 1.0, 24],\n",
              "       [1.0, 0.0, 0.0, 1.0, 72],\n",
              "       [1.0, 0.0, 0.0, 1.0, 48],\n",
              "       ...,\n",
              "       [0.0, 1.0, 1.0, 0.0, 48],\n",
              "       [1.0, 0.0, 0.0, 1.0, 24],\n",
              "       [1.0, 0.0, 0.0, 1.0, 72]], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ4trFxG89I5",
        "outputId": "a9611aac-e6de-4358-a903-601a780287f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(23814, 877)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BduztvDs8_5h",
        "outputId": "a66017aa-a8a5-42b1-92d4-dc778851eae6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.0, 0.0, 0.0, ..., 0.2139, 0.3801, 0.4176],\n",
              "       [1.0, 0.0, 0.0, ..., 0.1241, 0.6077, 0.7371],\n",
              "       [1.0, 0.0, 0.0, ..., -0.2187, -1.408, 0.6931],\n",
              "       ...,\n",
              "       [0.0, 1.0, 1.0, ..., 0.7592, 0.6656, 0.3808],\n",
              "       [1.0, 0.0, 0.0, ..., 0.7015, -0.629, 0.074],\n",
              "       [1.0, 0.0, 0.0, ..., -0.4775, -2.15, -4.252]], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gv_um0ir80qh"
      },
      "source": [
        "# Normalization\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X = sc.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXUjhudq9D7R",
        "outputId": "b980ea6b-6bfc-49af-c0b5-6e997400d70c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(23814, 877)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ETz6lfK9FXZ",
        "outputId": "f2a6cb8e-a9d4-43ba-f146-1e22150a8289",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(23814, 206)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOlODXI39HcR",
        "outputId": "0d4e5262-19a2-42b1-dff4-ee149a5cc7c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, num_features, num_targets, hidden_size):\n",
        "        super(Model, self).__init__()\n",
        "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n",
        "        \n",
        "        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n",
        "        \n",
        "        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n",
        "        self.dropout3 = nn.Dropout(0.5)\n",
        "        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.batch_norm1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.dense1(x))\n",
        "        \n",
        "        x = self.batch_norm2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = F.relu(self.dense2(x))\n",
        "        \n",
        "        x = self.batch_norm3(x)\n",
        "        x = self.dropout3(x)\n",
        "        x = self.dense3(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "# Vou inicializar no train\n",
        "model = Model(877, 206, 256)\n",
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (batch_norm1): BatchNorm1d(877, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (dropout1): Dropout(p=0.2, inplace=False)\n",
              "  (dense1): Linear(in_features=877, out_features=256, bias=True)\n",
              "  (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (dropout2): Dropout(p=0.5, inplace=False)\n",
              "  (dense2): Linear(in_features=256, out_features=256, bias=True)\n",
              "  (batch_norm3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (dropout3): Dropout(p=0.5, inplace=False)\n",
              "  (dense3): Linear(in_features=256, out_features=206, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgFUzvL9-0n3"
      },
      "source": [
        "# train_loader = torch.utils.data.DataLoader(\n",
        "#     train_dataset, batch_size=1024, num_workers=8\n",
        "# )\n",
        "\n",
        "def train(self, data_loader):\n",
        "    self.model.train()\n",
        "    final_loss = 0\n",
        "    for data in data_loader:\n",
        "        self.optimizer.zero_grad()\n",
        "        inputs = data[\"x\"].to(self.device)\n",
        "        targets = data[\"x\"].to(self.device)\n",
        "        outputs = self.model(inputs)\n",
        "        loss = self.loss_fn(targets, outputs)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        final_loss += loss.item()\n",
        "    return final_loss / len(data_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mr80eDF6qOd",
        "outputId": "13ac7009-42a6-47ec-971a-33a3765a1fa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "class Engine:\n",
        "    def __init__(self, model, optimizer, device):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.device = device\n",
        "\n",
        "    @staticmethod\n",
        "    def loss_fn(targets, outputs):\n",
        "        return nn.BCEWithLogitsLoss()(outputs, targets)\n",
        "\n",
        "    def train(self, data_loader):\n",
        "        self.model.train()\n",
        "        final_loss = 0\n",
        "        for data in data_loader:\n",
        "            self.optimizer.zero_grad()\n",
        "            inputs = data[\"x\"].to(self.device)\n",
        "            targets = data[\"x\"].to(self.device)\n",
        "            outputs = self.model(inputs)\n",
        "            loss = self.loss_fn(targets, outputs)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            final_loss += loss.item()\n",
        "        return final_loss / len(data_loader)\n",
        "\n",
        "\n",
        "# Train\n",
        "DEVICE = \"cuda\"\n",
        "EPOCHS = 100\n",
        "\n",
        "def run_training():\n",
        "    # df = pd.read_csv(\"train_features.csv\")\n",
        "    # df = utils.process_data(df)\n",
        "    # folds = pd.read_csv(\"train_folds.csv\")\n",
        "\n",
        "    # targets = folds.drop([\"sig_id\", \"kfold\"], axis=1).columns\n",
        "    # features = df.drop(\"sig_id\", axis=1).columns\n",
        "\n",
        "    # df = df.merge(folds, on=\"sig_id\", how=\"left\")\n",
        "\n",
        "    # train_df = df[df.kfold != fold].reset_index(drop=True) \n",
        "    # valid_df = df[df.kfold == fold].reset_index(drop=True) \n",
        "\n",
        "    # x_train = train_df[features].to_array()\n",
        "    # x_valid = valid_df[features].to_array()\n",
        "\n",
        "    # y_train = train_df[targets].to_array()\n",
        "    # y_valid = valid_df[targets].to_array()\n",
        "\n",
        "    # train_dataset = utils.MoaDataset(x_train, y_train)\n",
        "    # train_loader = torch.utils.data.DataLoader(\n",
        "    #     train_dataset, batch_size=1024, num_workers=8\n",
        "    # )\n",
        "\n",
        "    model = Model(877, 206)\n",
        "    model.to(DEVICE)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "    # scheduler = \n",
        "\n",
        "    eng = Engine(\n",
        "        model, optimizer, DEVICE\n",
        "    )\n",
        "\n",
        "    for _ in range(EPOCHS):\n",
        "        train_loss = eng.train(train_loss)\n",
        "        # valid_loss = ......\n",
        "\n",
        "run_training()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-5b8fe3d71193>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# valid_loss = ......\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-93-5b8fe3d71193>\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;31m# valid_loss = ......\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'train_loss' referenced before assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxmWBsSP8s6g"
      },
      "source": [
        "# loss_fn = nn.BCEWithLogitsLoss()                       \n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# batch_size = 64\n",
        "# n_epochs = 100\n",
        "# batch_no = len(X) // batch_size                 # batch_no=125, pois 125 * 64 = 8000\n",
        "\n",
        "# train_loss = 0\n",
        "# train_loss_min = np.Inf\n",
        "# for epoch in range(n_epochs):\n",
        "#     for i in range(batch_no):\n",
        "#         start = i * batch_size\n",
        "#         end = start + batch_size\n",
        "#         x_var = Variable(torch.FloatTensor(X[start:end]))\n",
        "#         y_var = Variable(torch.FloatTensor(y[start:end])) \n",
        "        \n",
        "        \n",
        "#         # output = model(x_var)\n",
        "#         # loss = loss_fn(output,y_var)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         output = model(x_var)       \n",
        "#         loss = loss_fn(output,y_var)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "        \n",
        "#         # values, labels = torch.max(output, 1)\n",
        "#         # num_right = np.sum(labels.data.numpy() == y[start:end])\n",
        "#         train_loss += loss.item() * batch_size\n",
        "    \n",
        "#     train_loss = train_loss / len(X)\n",
        "#     if train_loss <= train_loss_min:\n",
        "#         print(\"Validation loss decreased ({:6f} ===> {:6f}). Saving the model...\".format(train_loss_min,train_loss))\n",
        "#         torch.save(model.state_dict(), \"model.pt\")\n",
        "#         train_loss_min = train_loss\n",
        "    \n",
        "#     # if epoch % 200 == 0:\n",
        "#     #     print('')\n",
        "#     #     print(\"Epoch: {} \\tTrain Loss: {} \\tTrain Accuracy: {}\".format(epoch + 1, train_loss))\n",
        "#     print(\"Ronaldo\")\n",
        "\n",
        "# print('Training Ended! ')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}